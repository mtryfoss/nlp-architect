{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# NLP Architect - Intent Extraction tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's start by importing all the important classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n",
      "/usr/local/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nlp_architect.models.intent_extraction import MultiTaskIntentModel\n",
    "from nlp_architect.data.intent_datasets import SNIPS\n",
    "from nlp_architect.utils.embedding import load_word_embeddings\n",
    "from nlp_architect.utils.metrics import get_conll_scores\n",
    "from nlp_architect.utils.generic import one_hot\n",
    "\n",
    "from tensorflow.python.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Preparing the data\n",
    "The first step is the download the dataset into a folder and load the data into the memory\n",
    "using the `SNIPS` data loader.\n",
    "\n",
    "### SNIPS NLU Benchmark dataset\n",
    "\n",
    "SNIPS dataset has 7 types of intents:\n",
    "- ‘Add to playlist’\n",
    "- ‘Rate book’\n",
    "- ‘Check weather’\n",
    "- ‘Play music’\n",
    "- ‘Book restaurant’\n",
    "- ‘Search event’\n",
    "- ‘Search art’\n",
    "\n",
    "73 types of labels (including `B-` and `I-` prefixed labels), train/test set sizes: ~14000/700\n",
    "\n",
    "More info: [here](https://github.com/snipsco/nlu-benchmark)\n",
    "\n",
    "(The terms and conditions of the data set license apply. Intel does not grant any rights to the data files)\n",
    "\n",
    "Git clone the repository with the dataset:\n",
    "```\n",
    "git clone https://github.com/snipsco/nlu-benchmark.git\n",
    "```\n",
    "\n",
    "Point the source of the dataset to `nlu-benchmark/2017-06-custom-intent-engines/` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_length = 50\n",
    "word_length = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_path = 'nlu-benchmark/2017-06-custom-intent-engines/'\n",
    "dataset = SNIPS(path=dataset_path,\n",
    "                sentence_length=sentence_length,\n",
    "                word_length=word_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset is loaded, we can extract the ready made `train` and `test` sets. Each set is made up of a tuple of 4 elements:\n",
    "- Words (`train_x` and `test_x`)\n",
    "- Word character representation (`train_c` and `test_c`)\n",
    "- Intent type (`train_i` and `test_i`)\n",
    "- Token slot tags (`train_y` and `test_y`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x, train_c, train_i, train_y = dataset.train_set\n",
    "test_x, test_c, test_i, test_y = dataset.test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13784, 50), (13784, 50, 12), (13784,), (13784, 50))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_c.shape, train_i.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences are encoded in sparse int representation (str->int vocabularies stored in the dataset object) as NumPy arrary.\n",
    "Lets look at the sentence in index 5544, translate it back to strings so we could read the sentence, and look at the encoded label tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1261,  103,    6, 5286, 5295,  263, 2228, 3331, 3310,    5, 6370,\n",
       "       5325, 2523, 1250,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[5544]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'is',\n",
       " 'the',\n",
       " 'weather',\n",
       " 'forecast',\n",
       " 'for',\n",
       " 'four',\n",
       " 'pm',\n",
       " 'close',\n",
       " 'to',\n",
       " 'stretch',\n",
       " 'point',\n",
       " 'state',\n",
       " 'park']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dataset.word_vocab.id_to_word(i) for i in train_x[5544] if dataset.word_vocab.id_to_word(i) is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-timeRange',\n",
       " 'I-timeRange',\n",
       " 'B-spatial_relation',\n",
       " 'O',\n",
       " 'B-geographic_poi',\n",
       " 'I-geographic_poi',\n",
       " 'I-geographic_poi',\n",
       " 'I-geographic_poi']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dataset.tags_vocab.id_to_word(i) for i in train_y[5544] if dataset.tags_vocab.id_to_word(i) is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### External word embedding\n",
    "\n",
    "Now it's time to load the external word embedding model.\n",
    "We'll use `load_word_embeddings` function that reads the file and loads up the words into numpy arrays.\n",
    "Once done, we'll create a 2D array with the words we have in our dataset word lexicon - we'll save it in `embedding_matrix` and we'll use it later when we load the embedding layer of the words.\n",
    "\n",
    "You can download the GloVe word embedding models from [here](https://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    "(The terms and conditions of the data set license apply. Intel does not grant any rights to the data files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11975, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlp_architect.utils.embedding import get_embedding_matrix\n",
    "\n",
    "embedding_path = 'glove.6B.100d.txt'\n",
    "embedding_size = 100\n",
    "\n",
    "embedding_model, _ = load_word_embeddings(embedding_path)\n",
    "embedding_mat = get_embedding_matrix(embedding_model, dataset.word_vocab)\n",
    "\n",
    "embedding_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Building the network\n",
    "\n",
    "Now for the fun part, let's start by defining the parameters of the network we're going to build, such as, the LSTM layer's hidden state, the number of output labels and intents to predict and the size of the character embedding vectors.\n",
    "\n",
    "The network topology looks as the following diagram\n",
    "\n",
    "### High level topology\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "This network is defined in `nlp_architect.models.intent_extraction` packages as `MultiTaskIntentModel`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first convert the slot labels an intent classifications into 1-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = to_categorical(test_y, dataset.label_vocab_size)\n",
    "train_y = to_categorical(train_y, dataset.label_vocab_size)\n",
    "train_i = one_hot(train_i, len(dataset.intents_vocab))\n",
    "test_i = one_hot(test_i, len(dataset.intents_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the input and output data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = [train_x, train_c]\n",
    "train_outs = [train_i, train_y]\n",
    "test_inputs = [test_x, test_c]\n",
    "test_outs = [test_i, test_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initiate the model object and build the network with the defined parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 13:47:54.947280: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2023-04-27 13:47:54.963468: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe5b6ea55a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-27 13:47:54.963481: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "model = MultiTaskIntentModel()\n",
    "model.build(dataset.word_len,\n",
    "            dataset.label_vocab_size,\n",
    "            dataset.intent_size,\n",
    "            dataset.word_vocab_size-1,\n",
    "            dataset.char_vocab_size,\n",
    "            word_emb_dims=embedding_size,\n",
    "            tagger_lstm_dims=100,\n",
    "            dropout=0.2)\n",
    "model.load_embedding_weights(embedding_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "We've got a model, it's time to train the network.\n",
    "\n",
    "We define the batch size and the number of epochs to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13784 samples, validate on 700 samples\n",
      "Epoch 1/5\n",
      "13784/13784 [==============================] - 100s 7ms/sample - loss: 17.9649 - intent_classifier_output_loss: 0.5828 - intent_slot_crf_loss: 17.3754 - intent_classifier_output_categorical_accuracy: 0.8284 - intent_slot_crf_accuracy: 0.9305 - val_loss: 158.1009 - val_intent_classifier_output_loss: 0.1084 - val_intent_slot_crf_loss: 157.9917 - val_intent_classifier_output_categorical_accuracy: 0.9743 - val_intent_slot_crf_accuracy: 0.9713\n",
      "Epoch 2/5\n",
      "13784/13784 [==============================] - 90s 7ms/sample - loss: 3.4879 - intent_classifier_output_loss: 0.1039 - intent_slot_crf_loss: 3.3836 - intent_classifier_output_categorical_accuracy: 0.9716 - intent_slot_crf_accuracy: 0.9805 - val_loss: 155.2761 - val_intent_classifier_output_loss: 0.0691 - val_intent_slot_crf_loss: 155.2051 - val_intent_classifier_output_categorical_accuracy: 0.9800 - val_intent_slot_crf_accuracy: 0.9865\n",
      "Epoch 3/5\n",
      "13784/13784 [==============================] - 88s 6ms/sample - loss: 1.6572 - intent_classifier_output_loss: 0.0704 - intent_slot_crf_loss: 1.5869 - intent_classifier_output_categorical_accuracy: 0.9798 - intent_slot_crf_accuracy: 0.9904 - val_loss: 153.8837 - val_intent_classifier_output_loss: 0.0426 - val_intent_slot_crf_loss: 153.8403 - val_intent_classifier_output_categorical_accuracy: 0.9886 - val_intent_slot_crf_accuracy: 0.9903\n",
      "Epoch 4/5\n",
      "13784/13784 [==============================] - 87s 6ms/sample - loss: 1.0317 - intent_classifier_output_loss: 0.0482 - intent_slot_crf_loss: 0.9832 - intent_classifier_output_categorical_accuracy: 0.9861 - intent_slot_crf_accuracy: 0.9937 - val_loss: 152.9562 - val_intent_classifier_output_loss: 0.0350 - val_intent_slot_crf_loss: 152.9200 - val_intent_classifier_output_categorical_accuracy: 0.9886 - val_intent_slot_crf_accuracy: 0.9927\n",
      "Epoch 5/5\n",
      "13784/13784 [==============================] - 90s 7ms/sample - loss: 0.7101 - intent_classifier_output_loss: 0.0380 - intent_slot_crf_loss: 0.6720 - intent_classifier_output_categorical_accuracy: 0.9902 - intent_slot_crf_accuracy: 0.9955 - val_loss: 152.3650 - val_intent_classifier_output_loss: 0.0376 - val_intent_slot_crf_loss: 152.3264 - val_intent_classifier_output_categorical_accuracy: 0.9871 - val_intent_slot_crf_accuracy: 0.9921\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "no_epochs = 5\n",
    "\n",
    "# train the model\n",
    "model.fit(train_inputs, train_outs,\n",
    "          batch_size=batch_size,\n",
    "          epochs=no_epochs,\n",
    "          validation=(test_inputs, test_outs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and evaluating the network\n",
    "Great! we have a trained model, let's check how well it performs.\n",
    "\n",
    "First, we need to run all the test data through the network and get the network's preditions. Once done, we can use `get_conll_scores` to get the actual CONLLEVAL benchmark results on the test data (in terms of precision/recall/F1 and per label type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict([test_x, test_c], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((700, 9), (700, 50, 75))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].shape, predictions[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval = get_conll_scores(predictions, test_y,\n",
    "                            {v: k for k, v in dataset.tags_vocab.vocab.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      "               best_rating      0.981     1.000     0.990        51\n",
      "                 timeRange      0.920     0.945     0.933       110\n",
      "           restaurant_type      0.924     0.984     0.953        62\n",
      "          spatial_relation      0.957     0.985     0.971        68\n",
      "              rating_value      0.990     1.000     0.995       100\n",
      "                    artist      0.884     0.908     0.896       109\n",
      "               object_name      0.924     0.967     0.945       151\n",
      "                     state      0.943     0.980     0.962        51\n",
      "      object_location_type      1.000     1.000     1.000        20\n",
      "                   service      1.000     0.974     0.987        39\n",
      "                  facility      1.000     1.000     1.000         7\n",
      "                   cuisine      1.000     0.909     0.952        11\n",
      "               object_type      1.000     0.994     0.997       156\n",
      "               rating_unit      1.000     1.000     1.000        61\n",
      "             object_select      0.959     0.959     0.959        49\n",
      "                  playlist      0.919     0.936     0.927       109\n",
      "                music_item      0.882     0.953     0.916        86\n",
      "                   country      1.000     0.977     0.989        44\n",
      "     condition_temperature      1.000     1.000     1.000        21\n",
      "         party_size_number      1.000     1.000     1.000        57\n",
      "                      city      0.971     0.930     0.950        71\n",
      "               served_dish      0.625     1.000     0.769         5\n",
      "                movie_name      0.930     0.816     0.870        49\n",
      "           restaurant_name      1.000     0.950     0.974        20\n",
      "             location_name      1.000     1.000     1.000        29\n",
      "          current_location      0.944     1.000     0.971        17\n",
      "               entity_name      0.500     0.611     0.550        18\n",
      "            playlist_owner      1.000     1.000     1.000        54\n",
      "object_part_of_series_type      1.000     1.000     1.000        15\n",
      "            geographic_poi      0.875     0.875     0.875        16\n",
      "                      sort      0.962     0.962     0.962        26\n",
      "     condition_description      1.000     0.909     0.952        22\n",
      "                     track      0.214     0.500     0.300         6\n",
      "                      year      1.000     1.000     1.000        25\n",
      "                     album      0.000     0.000     0.000        13\n",
      "                movie_type      1.000     1.000     1.000        24\n",
      "    party_size_description      1.000     1.000     1.000        13\n",
      "                       poi      0.833     0.833     0.833         6\n",
      "                     genre      0.667     0.667     0.667         3\n",
      "\n",
      "                 micro avg      0.941     0.952     0.947      1794\n",
      "                 macro avg      0.941     0.952     0.946      1794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per label performance breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intent classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9871428571428571"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predicted_intents = predictions[0].argmax(1)\n",
    "truth_intents = test_i.argmax(1)\n",
    "accuracy_score(truth_intents, predicted_intents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using GloVe 300 word embedding model and 50+ epochs of training should produce a model with:\n",
    "\n",
    "- Intent detection: >99 F1\n",
    "- Slot label classification: >95 F1\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"mtryfoss_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"mtryfoss_model_info\", \"wb\") as fp:\n",
    "    info = {\n",
    "        \"type\": \"mtl\",\n",
    "        \"tags_vocab\": dataset.tags_vocab.vocab,\n",
    "        \"word_vocab\": dataset.word_vocab.vocab,\n",
    "        \"char_vocab\": dataset.char_vocab.vocab,\n",
    "        \"intent_vocab\": dataset.intents_vocab.vocab,\n",
    "    }\n",
    "    pickle.dump(info, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
